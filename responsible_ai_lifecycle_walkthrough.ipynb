{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-end Responsible AI lifecycle walkthrough"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to walk you through a concrete use case by following the three phases of the [ML workflow](https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/) and applying the most prominent recommendations from the Responsible AI lifecycle at each stage. This will be done in a cloud-native manner by leveraging [Azure ML MLOps capabilities]().\n",
        "\n",
        "This use case uses the well-known [UCI adult census dataset](https://archive.ics.uci.edu/ml/datasets/Adult). For our purposes, we will use treat this as a loan decision classification problem. We will pretend that the label indicates whether each individual repaid a loan in the past. We will use the data to train a predictor to predict whether previously unseen individuals will repay a loan or not. The assumption is that the model predictions will be used to decide whether an individual should be offered a loan.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to your Azure ML workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abdou-azure-ml-workspace\tfrancecentral\tabdou-resources-group\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell creates a workspace object from the existing workspace. ``Workspace.from_config()`` reads the file ``config.json`` and loads the details into an object named ``ws``. The compute instance has a copy of this file saved in its root directory. If you run the code elsewhere, you'll need to [create the file](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading\n",
        "\n",
        "We use the **adult census** dataset that we collect throught the **shap** library. Let's load and have a first look at the data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "import shap # Data is collected through the shap library\n",
        "import pandas as pd\n",
        "\n",
        "# Load the adult cencus dataset\n",
        "X_raw, Y = shap.datasets.adult()\n",
        "df = pd.DataFrame(X_raw, Y)\n",
        "print (\"X_raw shape:\", X_raw.shape)\n",
        "X_raw.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_raw shape: (32561, 12)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Age  Workclass  Education-Num  Marital Status  Occupation  Relationship  \\\n",
              "0  39.0          7           13.0               4           1             0   \n",
              "1  50.0          6           13.0               2           4             4   \n",
              "2  38.0          4            9.0               0           6             0   \n",
              "3  53.0          4            7.0               2           6             4   \n",
              "4  28.0          4           13.0               2          10             5   \n",
              "\n",
              "   Race  Sex  Capital Gain  Capital Loss  Hours per week  Country  \n",
              "0     4    1        2174.0           0.0            40.0       39  \n",
              "1     4    1           0.0           0.0            13.0       39  \n",
              "2     4    1           0.0           0.0            40.0       39  \n",
              "3     2    1           0.0           0.0            40.0       39  \n",
              "4     2    0           0.0           0.0            40.0        5  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Education-Num</th>\n",
              "      <th>Marital Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Capital Gain</th>\n",
              "      <th>Capital Loss</th>\n",
              "      <th>Hours per week</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39.0</td>\n",
              "      <td>7</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50.0</td>\n",
              "      <td>6</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38.0</td>\n",
              "      <td>4</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28.0</td>\n",
              "      <td>4</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1629468662606
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing and cleaning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identifying and handling the missing values"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "# Number of missing values over all columns\n",
        "X_raw.isna().sum().sum()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "All features above look numeric, however some of them are just \"numeric codes\" and the features they represent are rather categorical. \n",
        "So for more accurate results, we separate categorical features from “real” numeric ones."
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import numpy as np\n",
        "print(X_raw.dtypes)\n",
        "categorical_features_indices = np.where(np.logical_or(X_raw.dtypes == np.int8, X_raw.dtypes == np.int32))[0]\n",
        "\n",
        "print('categorical_features_indices:',categorical_features_indices)\n",
        "\n",
        "numeric_features_indices = np.where(X_raw.dtypes == np.float32)[0]\n",
        "numeric_features_indices\n",
        "print('numeric_features_indices:',numeric_features_indices)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "column_transformer = ColumnTransformer ([\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'),\n",
        "    categorical_features_indices),\n",
        "    ('scaler', StandardScaler(),\n",
        "    numeric_features_indices)\n",
        "])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age               float32\n",
            "Workclass            int8\n",
            "Education-Num     float32\n",
            "Marital Status       int8\n",
            "Occupation           int8\n",
            "Relationship        int64\n",
            "Race                 int8\n",
            "Sex                  int8\n",
            "Capital Gain      float32\n",
            "Capital Loss      float32\n",
            "Hours per week    float32\n",
            "Country              int8\n",
            "dtype: object\n",
            "categorical_features_indices: [ 1  3  4  6  7 11]\n",
            "numeric_features_indices: [ 0  2  8  9 10]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "print(\"Before label encoding:\",Y) # --> [False False False  ... False False True]\n",
        "Y=le.fit_transform(Y)\n",
        "print(\"After label encoding:\",Y) # --> [0 0 0  ... 0 0 1]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before label encoding: [False False False ... False False  True]\n",
            "After label encoding: [0 0 0 ... 0 0 1]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data split and Features enrichment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "A=X_raw[['Sex', 'Race']]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(\n",
        "    X_raw, Y, A,\n",
        "    test_size=0.2, random_state=0, stratify=Y)\n",
        "\n",
        "X_train.reset_index(drop=True)\n",
        "X_test.reset_index(drop=True)\n",
        "A_train.reset_index(drop=True)\n",
        "A_test.reset_index(drop=True)\n",
        "\n",
        "print(\"X_raw shape: {}, X_train shape: {}, X_test shape: {}\".format(\n",
        "    X_raw.shape, X_train.shape, X_test.shape))\n",
        "    \n",
        "# test dataframe: features enrichment\n",
        "import pandas as pd\n",
        "\n",
        "pandas_warnings=pd.get_option('mode.chained_assignment')\n",
        "# to avoid warning 'A value is trying to be set on a copy of a slice from a DataFrame'\n",
        "\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "# improve labels by replacing numbers with labels\n",
        "A_test.Sex.loc[(A_test['Sex']==0)] = 'female'\n",
        "A_test.Sex.loc[(A_test['Sex']==1)] = 'male'\n",
        "\n",
        "A_test.Race.loc[(A_test['Race']==0)] = 'Amer-Indian-Eskimo'\n",
        "A_test.Race.loc[(A_test['Race']==1)] = 'Asian-Pac-Islander'\n",
        "A_test.Race.loc[(A_test['Race']==2)] = 'Black'\n",
        "A_test.Race.loc[(A_test['Race']==3)] = 'Other'\n",
        "A_test.Race.loc[(A_test['Race']==4)] = 'White'\n",
        "\n",
        "pd.set_option('mode.chained_assignment', pandas_warnings)\n",
        "\n",
        "A_test.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_raw shape: (32561, 12), X_train shape: (26048, 12), X_test shape: (6513, 12)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Sex                Race\n",
              "13077    male               White\n",
              "25002    male  Asian-Pac-Islander\n",
              "23777  female               White\n",
              "71     female               Black\n",
              "955      male               White"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Race</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13077</th>\n",
              "      <td>male</td>\n",
              "      <td>White</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25002</th>\n",
              "      <td>male</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23777</th>\n",
              "      <td>female</td>\n",
              "      <td>White</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>female</td>\n",
              "      <td>Black</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955</th>\n",
              "      <td>male</td>\n",
              "      <td>White</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Train your first classification model with Logistic Regression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', column_transformer),\n",
        "    ('classifier_LR', LogisticRegression(solver='liblinear', fit_intercept=True))])\n",
        "\n",
        "unmitigated_predictor1 = clf.fit(X_train, Y_train)\n",
        "print('unmitigated_predictor1.score:', unmitigated_predictor1.score(X_test, Y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unmitigated_predictor1.score: 0.8461538461538461\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Train your second classification model with SVM\n",
        "from sklearn import svm\n",
        "svm_predictor = svm.SVC()\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', column_transformer),\n",
        "    ('classifier_SVM', svm_predictor)])\n",
        "\n",
        "unmitigated_predictor2 = clf.fit(X_train, Y_train)\n",
        "print('unmitigated_predictor2.score:', unmitigated_predictor2.score(X_test, Y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unmitigated_predictor2.score: 0.8509135575003839\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CatBoost Classifier"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# !pip install catboost"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Train your third classification model with Catboost Classifier\n",
        "from catboost import CatBoostClassifier # !pip install catboost==0.18.1\n",
        "\n",
        "cbc = CatBoostClassifier(\n",
        "    random_seed=42, logging_level=\"Silent\", iterations=150)\n",
        "\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', column_transformer),\n",
        "    ('classifier_CBC', cbc)])\n",
        "\n",
        "unmitigated_predictor3 = clf.fit(X_train, \n",
        "                                 Y_train\n",
        "                                 #classifier_CBC__eval_set(X_test, Y_test)\n",
        "                                 #classifier_CBC__cat_features=categorical_features_indices\n",
        "                                )\n",
        "print('unmitigated_predictor3.score:', unmitigated_predictor3.score(X_test, Y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unmitigated_predictor3.score: 0.873637340703209\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge the trained models into an Array"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "unmitigated_predictors=[]\n",
        "unmitigated_predictors.append(unmitigated_predictor1)\n",
        "unmitigated_predictors.append(unmitigated_predictor2)\n",
        "unmitigated_predictors.append(unmitigated_predictor3)\n",
        "unmitigated_predictors"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Pipeline(memory=None,\n",
              "          steps=[('preprocessor',\n",
              "                  ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                    sparse_threshold=0.3,\n",
              "                                    transformer_weights=None,\n",
              "                                    transformers=[('onehot',\n",
              "                                                   OneHotEncoder(categories='auto',\n",
              "                                                                 drop=None,\n",
              "                                                                 dtype=<class 'numpy.float64'>,\n",
              "                                                                 handle_unknown='ignore',\n",
              "                                                                 sparse=True),\n",
              "                                                   array([ 1,  3,  4,  6,  7, 11])),\n",
              "                                                  ('scaler',\n",
              "                                                   StandardScaler(copy=True,\n",
              "                                                                  with_mean=True,\n",
              "                                                                  with_std=True),\n",
              "                                                   array([ 0,  2,  8,  9, 10]))],\n",
              "                                    verbose=False)),\n",
              "                 ('classifier_LR',\n",
              "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                     fit_intercept=True, intercept_scaling=1,\n",
              "                                     l1_ratio=None, max_iter=100,\n",
              "                                     multi_class='auto', n_jobs=None,\n",
              "                                     penalty='l2', random_state=None,\n",
              "                                     solver='liblinear', tol=0.0001, verbose=0,\n",
              "                                     warm_start=False))],\n",
              "          verbose=False),\n",
              " Pipeline(memory=None,\n",
              "          steps=[('preprocessor',\n",
              "                  ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                    sparse_threshold=0.3,\n",
              "                                    transformer_weights=None,\n",
              "                                    transformers=[('onehot',\n",
              "                                                   OneHotEncoder(categories='auto',\n",
              "                                                                 drop=None,\n",
              "                                                                 dtype=<class 'numpy.float64'>,\n",
              "                                                                 handle_unknown='ignore',\n",
              "                                                                 sparse=True),\n",
              "                                                   array([ 1,  3,  4,  6,  7, 11])),\n",
              "                                                  ('scaler',\n",
              "                                                   StandardScaler(copy=True,\n",
              "                                                                  with_mean=True,\n",
              "                                                                  with_std=True),\n",
              "                                                   array([ 0,  2,  8,  9, 10]))],\n",
              "                                    verbose=False)),\n",
              "                 ('classifier_SVM',\n",
              "                  SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                      coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                      gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                      probability=False, random_state=None, shrinking=True,\n",
              "                      tol=0.001, verbose=False))],\n",
              "          verbose=False),\n",
              " Pipeline(memory=None,\n",
              "          steps=[('preprocessor',\n",
              "                  ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                    sparse_threshold=0.3,\n",
              "                                    transformer_weights=None,\n",
              "                                    transformers=[('onehot',\n",
              "                                                   OneHotEncoder(categories='auto',\n",
              "                                                                 drop=None,\n",
              "                                                                 dtype=<class 'numpy.float64'>,\n",
              "                                                                 handle_unknown='ignore',\n",
              "                                                                 sparse=True),\n",
              "                                                   array([ 1,  3,  4,  6,  7, 11])),\n",
              "                                                  ('scaler',\n",
              "                                                   StandardScaler(copy=True,\n",
              "                                                                  with_mean=True,\n",
              "                                                                  with_std=True),\n",
              "                                                   array([ 0,  2,  8,  9, 10]))],\n",
              "                                    verbose=False)),\n",
              "                 ('classifier_CBC',\n",
              "                  <catboost.core.CatBoostClassifier object at 0x7fb3aeb06c18>)],\n",
              "          verbose=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models registration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "from azureml.core.model import Model\n",
        "from joblib import dump\n",
        "\n",
        "registered_unmitigated_predictors=[]\n",
        "for trained_model in unmitigated_predictors:\n",
        "    model_name = list(trained_model.named_steps.keys())[-1]\n",
        "    model_name_toregister=model_name+trained_model.steps[-1][0]\n",
        "    model_path_local='./outputs/' + model_name_toregister + '.pkl'\n",
        "\n",
        "    dump(value=trained_model, filename=model_path_local)\n",
        "    registered_model = Model.register(\n",
        "        workspace=ws, \n",
        "        model_name=model_name_toregister, \n",
        "        model_path=model_path_local)\n",
        "    \n",
        "    registered_unmitigated_predictors.append(registered_model)\n",
        "    print(registered_model)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './outputs/classifier_LRclassifier_LR.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1e4faf4e297f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_path_local\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./outputs/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name_toregister\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     registered_model = Model.register(\n\u001b[1;32m     12\u001b[0m         \u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/classifier_LRclassifier_LR.pkl'"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "for unmitigated_predictor in unmitigated_predictors:\n",
        "    unmitigated_predictor=unmitigated_predictors[0]\n",
        "    Y_pred=unmitigated_predictor.predict(X_test)\n",
        "    print(Y_pred)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 1]\n",
            "[0 0 0 ... 0 0 1]\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the confusion matrix"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "conf_mx = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "# confusion matrix\n",
        "print(\"Confusion matrix:\\n\",conf_mx)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[4596  349]\n",
            " [ 653  915]]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "print(\"Confusion matrix:\\n\",conf_mx)\n",
        "\n",
        "true_positive  = conf_mx[0,0]\n",
        "true_negative  = conf_mx[1,1] \n",
        "false_negative = conf_mx[0,1]\n",
        "false_positive = conf_mx[1,0]\n",
        "\n",
        "total_positive = true_positive + false_negative\n",
        "total_negative = true_negative + false_positive\n",
        "total_population  = total_positive + total_negative\n",
        "\n",
        "recall      = true_positive/total_positive # also called Sensitivity or True Positive Rate\n",
        "specificity = true_negative/total_negative # also cald True Negative Rate\n",
        "accuracy    = (true_positive + true_negative) / total_population\n",
        "precision   = true_positive/(true_positive + false_positive)\n",
        "f1_score    = 2 * (precision*recall) / (precision+recall)\n",
        "\n",
        "print (\"Recall = Sensitivity = True Positive Rate =\", recall)\n",
        "print (\"Specificity = True Negative Rate =\", specificity)\n",
        "print (\"Accuracy =\", accuracy)\n",
        "print (\"Precision =\", precision)\n",
        "print (\"F1 Score=\", f1_score)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[4596  349]\n",
            " [ 653  915]]\n",
            "Recall = Sensitivity = True Positive Rate = 0.9294236602628918\n",
            "Specificity = True Negative Rate = 0.5835459183673469\n",
            "Accuracy = 0.8461538461538461\n",
            "Precision = 0.8755953514955229\n",
            "F1 Score= 0.9017068864037668\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "import sklearn.metrics as skm\n",
        "\n",
        "for unmitigated_predictor in unmitigated_predictors:\n",
        "    Y_pred=unmitigated_predictor.predict(X_test)\n",
        "    conf_mx = confusion_matrix(Y_test, Y_pred)\n",
        "    print(\"CLASSIFIER:\",unmitigated_predictor.steps[-1][0])\n",
        "    print(\"Confusion matrix:\\n\",skm.confusion_matrix(Y_test,Y_pred))\n",
        "\n",
        "    print(\"Recall: {}\\nAccuracy: {}\\nPrecision: {}\\nF1 Score: {}\\n\".format(\n",
        "        skm.recall_score(Y_test, Y_pred,average='binary'),\n",
        "        skm.accuracy_score(Y_test, Y_pred),\n",
        "        skm.precision_score(Y_test, Y_pred),\n",
        "        skm.f1_score(Y_test, Y_pred)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASSIFIER: classifier_LR\n",
            "Confusion matrix:\n",
            " [[4596  349]\n",
            " [ 653  915]]\n",
            "Recall: 0.5835459183673469\n",
            "Accuracy: 0.8461538461538461\n",
            "Precision: 0.7238924050632911\n",
            "F1 Score: 0.646186440677966\n",
            "\n",
            "CLASSIFIER: classifier_SVM\n",
            "Confusion matrix:\n",
            " [[4636  309]\n",
            " [ 662  906]]\n",
            "Recall: 0.5778061224489796\n",
            "Accuracy: 0.8509135575003839\n",
            "Precision: 0.745679012345679\n",
            "F1 Score: 0.651095939633489\n",
            "\n",
            "CLASSIFIER: classifier_CBC\n",
            "Confusion matrix:\n",
            " [[4677  268]\n",
            " [ 555 1013]]\n",
            "Recall: 0.6460459183673469\n",
            "Accuracy: 0.873637340703209\n",
            "Precision: 0.7907884465261514\n",
            "F1 Score: 0.7111267111267112\n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fairlearn dashboard"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "from fairlearn.widget import FairlearnDashboard\n",
        "import joblib, numpy as np\n",
        "\n",
        "# the following dict contains (<model_id>, <predictions>) pairs\n",
        "ys_pred = {}\n",
        "\n",
        "for rup in registered_unmitigated_predictors:\n",
        "    id=rup.id # extract <model_id> from registered models\n",
        "    model_name=rup.name\n",
        "    version=rup.version\n",
        "    model_path=Model.get_model_path(model_name=model_name, version=version, _workspace=ws)\n",
        "    unmitigated_predictor = joblib.load(model_path) # retrieve <predictions>\n",
        "    ys_pred[id]=unmitigated_predictor.predict(X_test)\n",
        "    \n",
        "FairlearnDashboard(\n",
        "    sensitive_features=A_test,\n",
        "    sensitive_feature_names=np.array(A_test.columns),\n",
        "    y_true=Y_test,\n",
        "    y_pred=ys_pred)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fairlearn.widget'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-90f11dd7c99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfairlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidget\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFairlearnDashboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the following dict contains (<model_id>, <predictions>) pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mys_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fairlearn.widget'"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}